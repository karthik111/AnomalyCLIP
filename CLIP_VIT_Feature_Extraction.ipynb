{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgntECIyk/VAJtyX0h8vxy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik111/AnomalyCLIP/blob/main/CLIP_VIT_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5ZXhOHPp2u-",
        "outputId": "b9d9f363-c1f8-4cbb-d160-2e2d8cb87ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from PIL import Image\n",
        "\n",
        "#Load CLIP model\n",
        "model = SentenceTransformer('clip-ViT-B-16')\n",
        "\n",
        "#Encode an image:\n",
        "#img_emb = model.encode(Image.open('2 dogs in snow.jpeg'))\n",
        "img_emb = model.encode(Image.open('assault1.jpeg'))\n",
        "\n",
        "#Encode text descriptions\n",
        "#text_emb = model.encode(['Two dogs in the snow', 'A cat on a table', 'A picture of London at night',\n",
        "#                         'Three dogs in the snow',\n",
        "#                         'Two dogs in snow'])\n",
        "\n",
        "text_emb = model.encode(['men in a room', 'a sleeping man', 'Men standing over another man',\n",
        "                         'A man being assaulted',\n",
        "                         'A room with people with one man lying down and others standing around him',\n",
        "                         'a room with ATM Machines'])\n",
        "\n",
        "#Compute cosine similarities\n",
        "cos_scores = util.cos_sim(img_emb, text_emb)\n",
        "print(cos_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_G3HMHmqG3_",
        "outputId": "70455b53-7cb3-47c9-ce65-e4d24f490379"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2250, 0.2127, 0.2258, 0.2522, 0.2241, 0.2185]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save('assault007_1200fr.npy', img_emb)"
      ],
      "metadata": {
        "id": "Sd2jVRrSvrrn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracting image features from the above clip-ViT-B-16 model"
      ],
      "metadata": {
        "id": "OFS_6BSCvqUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: extracting image features from the above clip-ViT-B-16 model\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from PIL import Image\n",
        "!pip install sentence-transformers\n",
        "\n",
        "#Load CLIP model\n",
        "model = SentenceTransformer('clip-ViT-B-16')\n",
        "\n",
        "# Assuming 'assault1.jpeg' is in the current directory or provide the full path\n",
        "try:\n",
        "    img_emb = model.encode(Image.open('assault1.jpeg'))\n",
        "    print(\"Image features extracted successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'assault1.jpeg' not found. Please ensure the image file exists in the current directory or provide the correct path.\")\n",
        "    exit() # or handle the error as appropriate for your use case\n",
        "\n",
        "\n",
        "#Encode text descriptions\n",
        "text_emb = model.encode(['men in a room', 'a sleeping man', 'Men standing over another man',\n",
        "                         'A man being assaulted',\n",
        "                         'A room with people with one man lying down and others standing around him',\n",
        "                         'a room with ATM Machines'])\n",
        "\n",
        "#Compute cosine similarities\n",
        "cos_scores = util.cos_sim(img_emb, text_emb)\n",
        "cos_scores"
      ],
      "metadata": {
        "id": "ex3XiNlzvZxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = model._first_module().model\n",
        "\n",
        "for name, layer in clip_model.visual.named_modules():\n",
        "    print(name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "Fpu3zRgMzhwj",
        "outputId": "d16f0587-c4df-4966-974d-2a303f30da1d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CLIPModel' object has no attribute 'visual'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e3812824d8ba>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclip_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CLIPModel' object has no attribute 'visual'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the SentenceTransformer model\n",
        "model = SentenceTransformer('clip-ViT-B-16')\n",
        "\n",
        "# Access the underlying PyTorch model\n",
        "clip_model = model._first_module().model\n",
        "\n",
        "# Print the structure of the model\n",
        "print(clip_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oifBFdjQzjfI",
        "outputId": "ecb1fea9-fbe9-47eb-ddc0-3aeefa650a3c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIPModel(\n",
            "  (text_model): CLIPTextTransformer(\n",
            "    (embeddings): CLIPTextEmbeddings(\n",
            "      (token_embedding): Embedding(49408, 512)\n",
            "      (position_embedding): Embedding(77, 512)\n",
            "    )\n",
            "    (encoder): CLIPEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x CLIPEncoderLayer(\n",
            "          (self_attn): CLIPSdpaAttention(\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): CLIPMLP(\n",
            "            (activation_fn): QuickGELUActivation()\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (vision_model): CLIPVisionTransformer(\n",
            "    (embeddings): CLIPVisionEmbeddings(\n",
            "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
            "      (position_embedding): Embedding(197, 768)\n",
            "    )\n",
            "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (encoder): CLIPEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x CLIPEncoderLayer(\n",
            "          (self_attn): CLIPSdpaAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): CLIPMLP(\n",
            "            (activation_fn): QuickGELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
            "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in clip_model.named_modules():\n",
        "    print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ab0dyrM0MiW",
        "outputId": "247e1f5a-0717-416d-d108-2d08fd440a7e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "text_model\n",
            "text_model.embeddings\n",
            "text_model.embeddings.token_embedding\n",
            "text_model.embeddings.position_embedding\n",
            "text_model.encoder\n",
            "text_model.encoder.layers\n",
            "text_model.encoder.layers.0\n",
            "text_model.encoder.layers.0.self_attn\n",
            "text_model.encoder.layers.0.self_attn.k_proj\n",
            "text_model.encoder.layers.0.self_attn.v_proj\n",
            "text_model.encoder.layers.0.self_attn.q_proj\n",
            "text_model.encoder.layers.0.self_attn.out_proj\n",
            "text_model.encoder.layers.0.layer_norm1\n",
            "text_model.encoder.layers.0.mlp\n",
            "text_model.encoder.layers.0.mlp.activation_fn\n",
            "text_model.encoder.layers.0.mlp.fc1\n",
            "text_model.encoder.layers.0.mlp.fc2\n",
            "text_model.encoder.layers.0.layer_norm2\n",
            "text_model.encoder.layers.1\n",
            "text_model.encoder.layers.1.self_attn\n",
            "text_model.encoder.layers.1.self_attn.k_proj\n",
            "text_model.encoder.layers.1.self_attn.v_proj\n",
            "text_model.encoder.layers.1.self_attn.q_proj\n",
            "text_model.encoder.layers.1.self_attn.out_proj\n",
            "text_model.encoder.layers.1.layer_norm1\n",
            "text_model.encoder.layers.1.mlp\n",
            "text_model.encoder.layers.1.mlp.activation_fn\n",
            "text_model.encoder.layers.1.mlp.fc1\n",
            "text_model.encoder.layers.1.mlp.fc2\n",
            "text_model.encoder.layers.1.layer_norm2\n",
            "text_model.encoder.layers.2\n",
            "text_model.encoder.layers.2.self_attn\n",
            "text_model.encoder.layers.2.self_attn.k_proj\n",
            "text_model.encoder.layers.2.self_attn.v_proj\n",
            "text_model.encoder.layers.2.self_attn.q_proj\n",
            "text_model.encoder.layers.2.self_attn.out_proj\n",
            "text_model.encoder.layers.2.layer_norm1\n",
            "text_model.encoder.layers.2.mlp\n",
            "text_model.encoder.layers.2.mlp.activation_fn\n",
            "text_model.encoder.layers.2.mlp.fc1\n",
            "text_model.encoder.layers.2.mlp.fc2\n",
            "text_model.encoder.layers.2.layer_norm2\n",
            "text_model.encoder.layers.3\n",
            "text_model.encoder.layers.3.self_attn\n",
            "text_model.encoder.layers.3.self_attn.k_proj\n",
            "text_model.encoder.layers.3.self_attn.v_proj\n",
            "text_model.encoder.layers.3.self_attn.q_proj\n",
            "text_model.encoder.layers.3.self_attn.out_proj\n",
            "text_model.encoder.layers.3.layer_norm1\n",
            "text_model.encoder.layers.3.mlp\n",
            "text_model.encoder.layers.3.mlp.activation_fn\n",
            "text_model.encoder.layers.3.mlp.fc1\n",
            "text_model.encoder.layers.3.mlp.fc2\n",
            "text_model.encoder.layers.3.layer_norm2\n",
            "text_model.encoder.layers.4\n",
            "text_model.encoder.layers.4.self_attn\n",
            "text_model.encoder.layers.4.self_attn.k_proj\n",
            "text_model.encoder.layers.4.self_attn.v_proj\n",
            "text_model.encoder.layers.4.self_attn.q_proj\n",
            "text_model.encoder.layers.4.self_attn.out_proj\n",
            "text_model.encoder.layers.4.layer_norm1\n",
            "text_model.encoder.layers.4.mlp\n",
            "text_model.encoder.layers.4.mlp.activation_fn\n",
            "text_model.encoder.layers.4.mlp.fc1\n",
            "text_model.encoder.layers.4.mlp.fc2\n",
            "text_model.encoder.layers.4.layer_norm2\n",
            "text_model.encoder.layers.5\n",
            "text_model.encoder.layers.5.self_attn\n",
            "text_model.encoder.layers.5.self_attn.k_proj\n",
            "text_model.encoder.layers.5.self_attn.v_proj\n",
            "text_model.encoder.layers.5.self_attn.q_proj\n",
            "text_model.encoder.layers.5.self_attn.out_proj\n",
            "text_model.encoder.layers.5.layer_norm1\n",
            "text_model.encoder.layers.5.mlp\n",
            "text_model.encoder.layers.5.mlp.activation_fn\n",
            "text_model.encoder.layers.5.mlp.fc1\n",
            "text_model.encoder.layers.5.mlp.fc2\n",
            "text_model.encoder.layers.5.layer_norm2\n",
            "text_model.encoder.layers.6\n",
            "text_model.encoder.layers.6.self_attn\n",
            "text_model.encoder.layers.6.self_attn.k_proj\n",
            "text_model.encoder.layers.6.self_attn.v_proj\n",
            "text_model.encoder.layers.6.self_attn.q_proj\n",
            "text_model.encoder.layers.6.self_attn.out_proj\n",
            "text_model.encoder.layers.6.layer_norm1\n",
            "text_model.encoder.layers.6.mlp\n",
            "text_model.encoder.layers.6.mlp.activation_fn\n",
            "text_model.encoder.layers.6.mlp.fc1\n",
            "text_model.encoder.layers.6.mlp.fc2\n",
            "text_model.encoder.layers.6.layer_norm2\n",
            "text_model.encoder.layers.7\n",
            "text_model.encoder.layers.7.self_attn\n",
            "text_model.encoder.layers.7.self_attn.k_proj\n",
            "text_model.encoder.layers.7.self_attn.v_proj\n",
            "text_model.encoder.layers.7.self_attn.q_proj\n",
            "text_model.encoder.layers.7.self_attn.out_proj\n",
            "text_model.encoder.layers.7.layer_norm1\n",
            "text_model.encoder.layers.7.mlp\n",
            "text_model.encoder.layers.7.mlp.activation_fn\n",
            "text_model.encoder.layers.7.mlp.fc1\n",
            "text_model.encoder.layers.7.mlp.fc2\n",
            "text_model.encoder.layers.7.layer_norm2\n",
            "text_model.encoder.layers.8\n",
            "text_model.encoder.layers.8.self_attn\n",
            "text_model.encoder.layers.8.self_attn.k_proj\n",
            "text_model.encoder.layers.8.self_attn.v_proj\n",
            "text_model.encoder.layers.8.self_attn.q_proj\n",
            "text_model.encoder.layers.8.self_attn.out_proj\n",
            "text_model.encoder.layers.8.layer_norm1\n",
            "text_model.encoder.layers.8.mlp\n",
            "text_model.encoder.layers.8.mlp.activation_fn\n",
            "text_model.encoder.layers.8.mlp.fc1\n",
            "text_model.encoder.layers.8.mlp.fc2\n",
            "text_model.encoder.layers.8.layer_norm2\n",
            "text_model.encoder.layers.9\n",
            "text_model.encoder.layers.9.self_attn\n",
            "text_model.encoder.layers.9.self_attn.k_proj\n",
            "text_model.encoder.layers.9.self_attn.v_proj\n",
            "text_model.encoder.layers.9.self_attn.q_proj\n",
            "text_model.encoder.layers.9.self_attn.out_proj\n",
            "text_model.encoder.layers.9.layer_norm1\n",
            "text_model.encoder.layers.9.mlp\n",
            "text_model.encoder.layers.9.mlp.activation_fn\n",
            "text_model.encoder.layers.9.mlp.fc1\n",
            "text_model.encoder.layers.9.mlp.fc2\n",
            "text_model.encoder.layers.9.layer_norm2\n",
            "text_model.encoder.layers.10\n",
            "text_model.encoder.layers.10.self_attn\n",
            "text_model.encoder.layers.10.self_attn.k_proj\n",
            "text_model.encoder.layers.10.self_attn.v_proj\n",
            "text_model.encoder.layers.10.self_attn.q_proj\n",
            "text_model.encoder.layers.10.self_attn.out_proj\n",
            "text_model.encoder.layers.10.layer_norm1\n",
            "text_model.encoder.layers.10.mlp\n",
            "text_model.encoder.layers.10.mlp.activation_fn\n",
            "text_model.encoder.layers.10.mlp.fc1\n",
            "text_model.encoder.layers.10.mlp.fc2\n",
            "text_model.encoder.layers.10.layer_norm2\n",
            "text_model.encoder.layers.11\n",
            "text_model.encoder.layers.11.self_attn\n",
            "text_model.encoder.layers.11.self_attn.k_proj\n",
            "text_model.encoder.layers.11.self_attn.v_proj\n",
            "text_model.encoder.layers.11.self_attn.q_proj\n",
            "text_model.encoder.layers.11.self_attn.out_proj\n",
            "text_model.encoder.layers.11.layer_norm1\n",
            "text_model.encoder.layers.11.mlp\n",
            "text_model.encoder.layers.11.mlp.activation_fn\n",
            "text_model.encoder.layers.11.mlp.fc1\n",
            "text_model.encoder.layers.11.mlp.fc2\n",
            "text_model.encoder.layers.11.layer_norm2\n",
            "text_model.final_layer_norm\n",
            "vision_model\n",
            "vision_model.embeddings\n",
            "vision_model.embeddings.patch_embedding\n",
            "vision_model.embeddings.position_embedding\n",
            "vision_model.pre_layrnorm\n",
            "vision_model.encoder\n",
            "vision_model.encoder.layers\n",
            "vision_model.encoder.layers.0\n",
            "vision_model.encoder.layers.0.self_attn\n",
            "vision_model.encoder.layers.0.self_attn.k_proj\n",
            "vision_model.encoder.layers.0.self_attn.v_proj\n",
            "vision_model.encoder.layers.0.self_attn.q_proj\n",
            "vision_model.encoder.layers.0.self_attn.out_proj\n",
            "vision_model.encoder.layers.0.layer_norm1\n",
            "vision_model.encoder.layers.0.mlp\n",
            "vision_model.encoder.layers.0.mlp.activation_fn\n",
            "vision_model.encoder.layers.0.mlp.fc1\n",
            "vision_model.encoder.layers.0.mlp.fc2\n",
            "vision_model.encoder.layers.0.layer_norm2\n",
            "vision_model.encoder.layers.1\n",
            "vision_model.encoder.layers.1.self_attn\n",
            "vision_model.encoder.layers.1.self_attn.k_proj\n",
            "vision_model.encoder.layers.1.self_attn.v_proj\n",
            "vision_model.encoder.layers.1.self_attn.q_proj\n",
            "vision_model.encoder.layers.1.self_attn.out_proj\n",
            "vision_model.encoder.layers.1.layer_norm1\n",
            "vision_model.encoder.layers.1.mlp\n",
            "vision_model.encoder.layers.1.mlp.activation_fn\n",
            "vision_model.encoder.layers.1.mlp.fc1\n",
            "vision_model.encoder.layers.1.mlp.fc2\n",
            "vision_model.encoder.layers.1.layer_norm2\n",
            "vision_model.encoder.layers.2\n",
            "vision_model.encoder.layers.2.self_attn\n",
            "vision_model.encoder.layers.2.self_attn.k_proj\n",
            "vision_model.encoder.layers.2.self_attn.v_proj\n",
            "vision_model.encoder.layers.2.self_attn.q_proj\n",
            "vision_model.encoder.layers.2.self_attn.out_proj\n",
            "vision_model.encoder.layers.2.layer_norm1\n",
            "vision_model.encoder.layers.2.mlp\n",
            "vision_model.encoder.layers.2.mlp.activation_fn\n",
            "vision_model.encoder.layers.2.mlp.fc1\n",
            "vision_model.encoder.layers.2.mlp.fc2\n",
            "vision_model.encoder.layers.2.layer_norm2\n",
            "vision_model.encoder.layers.3\n",
            "vision_model.encoder.layers.3.self_attn\n",
            "vision_model.encoder.layers.3.self_attn.k_proj\n",
            "vision_model.encoder.layers.3.self_attn.v_proj\n",
            "vision_model.encoder.layers.3.self_attn.q_proj\n",
            "vision_model.encoder.layers.3.self_attn.out_proj\n",
            "vision_model.encoder.layers.3.layer_norm1\n",
            "vision_model.encoder.layers.3.mlp\n",
            "vision_model.encoder.layers.3.mlp.activation_fn\n",
            "vision_model.encoder.layers.3.mlp.fc1\n",
            "vision_model.encoder.layers.3.mlp.fc2\n",
            "vision_model.encoder.layers.3.layer_norm2\n",
            "vision_model.encoder.layers.4\n",
            "vision_model.encoder.layers.4.self_attn\n",
            "vision_model.encoder.layers.4.self_attn.k_proj\n",
            "vision_model.encoder.layers.4.self_attn.v_proj\n",
            "vision_model.encoder.layers.4.self_attn.q_proj\n",
            "vision_model.encoder.layers.4.self_attn.out_proj\n",
            "vision_model.encoder.layers.4.layer_norm1\n",
            "vision_model.encoder.layers.4.mlp\n",
            "vision_model.encoder.layers.4.mlp.activation_fn\n",
            "vision_model.encoder.layers.4.mlp.fc1\n",
            "vision_model.encoder.layers.4.mlp.fc2\n",
            "vision_model.encoder.layers.4.layer_norm2\n",
            "vision_model.encoder.layers.5\n",
            "vision_model.encoder.layers.5.self_attn\n",
            "vision_model.encoder.layers.5.self_attn.k_proj\n",
            "vision_model.encoder.layers.5.self_attn.v_proj\n",
            "vision_model.encoder.layers.5.self_attn.q_proj\n",
            "vision_model.encoder.layers.5.self_attn.out_proj\n",
            "vision_model.encoder.layers.5.layer_norm1\n",
            "vision_model.encoder.layers.5.mlp\n",
            "vision_model.encoder.layers.5.mlp.activation_fn\n",
            "vision_model.encoder.layers.5.mlp.fc1\n",
            "vision_model.encoder.layers.5.mlp.fc2\n",
            "vision_model.encoder.layers.5.layer_norm2\n",
            "vision_model.encoder.layers.6\n",
            "vision_model.encoder.layers.6.self_attn\n",
            "vision_model.encoder.layers.6.self_attn.k_proj\n",
            "vision_model.encoder.layers.6.self_attn.v_proj\n",
            "vision_model.encoder.layers.6.self_attn.q_proj\n",
            "vision_model.encoder.layers.6.self_attn.out_proj\n",
            "vision_model.encoder.layers.6.layer_norm1\n",
            "vision_model.encoder.layers.6.mlp\n",
            "vision_model.encoder.layers.6.mlp.activation_fn\n",
            "vision_model.encoder.layers.6.mlp.fc1\n",
            "vision_model.encoder.layers.6.mlp.fc2\n",
            "vision_model.encoder.layers.6.layer_norm2\n",
            "vision_model.encoder.layers.7\n",
            "vision_model.encoder.layers.7.self_attn\n",
            "vision_model.encoder.layers.7.self_attn.k_proj\n",
            "vision_model.encoder.layers.7.self_attn.v_proj\n",
            "vision_model.encoder.layers.7.self_attn.q_proj\n",
            "vision_model.encoder.layers.7.self_attn.out_proj\n",
            "vision_model.encoder.layers.7.layer_norm1\n",
            "vision_model.encoder.layers.7.mlp\n",
            "vision_model.encoder.layers.7.mlp.activation_fn\n",
            "vision_model.encoder.layers.7.mlp.fc1\n",
            "vision_model.encoder.layers.7.mlp.fc2\n",
            "vision_model.encoder.layers.7.layer_norm2\n",
            "vision_model.encoder.layers.8\n",
            "vision_model.encoder.layers.8.self_attn\n",
            "vision_model.encoder.layers.8.self_attn.k_proj\n",
            "vision_model.encoder.layers.8.self_attn.v_proj\n",
            "vision_model.encoder.layers.8.self_attn.q_proj\n",
            "vision_model.encoder.layers.8.self_attn.out_proj\n",
            "vision_model.encoder.layers.8.layer_norm1\n",
            "vision_model.encoder.layers.8.mlp\n",
            "vision_model.encoder.layers.8.mlp.activation_fn\n",
            "vision_model.encoder.layers.8.mlp.fc1\n",
            "vision_model.encoder.layers.8.mlp.fc2\n",
            "vision_model.encoder.layers.8.layer_norm2\n",
            "vision_model.encoder.layers.9\n",
            "vision_model.encoder.layers.9.self_attn\n",
            "vision_model.encoder.layers.9.self_attn.k_proj\n",
            "vision_model.encoder.layers.9.self_attn.v_proj\n",
            "vision_model.encoder.layers.9.self_attn.q_proj\n",
            "vision_model.encoder.layers.9.self_attn.out_proj\n",
            "vision_model.encoder.layers.9.layer_norm1\n",
            "vision_model.encoder.layers.9.mlp\n",
            "vision_model.encoder.layers.9.mlp.activation_fn\n",
            "vision_model.encoder.layers.9.mlp.fc1\n",
            "vision_model.encoder.layers.9.mlp.fc2\n",
            "vision_model.encoder.layers.9.layer_norm2\n",
            "vision_model.encoder.layers.10\n",
            "vision_model.encoder.layers.10.self_attn\n",
            "vision_model.encoder.layers.10.self_attn.k_proj\n",
            "vision_model.encoder.layers.10.self_attn.v_proj\n",
            "vision_model.encoder.layers.10.self_attn.q_proj\n",
            "vision_model.encoder.layers.10.self_attn.out_proj\n",
            "vision_model.encoder.layers.10.layer_norm1\n",
            "vision_model.encoder.layers.10.mlp\n",
            "vision_model.encoder.layers.10.mlp.activation_fn\n",
            "vision_model.encoder.layers.10.mlp.fc1\n",
            "vision_model.encoder.layers.10.mlp.fc2\n",
            "vision_model.encoder.layers.10.layer_norm2\n",
            "vision_model.encoder.layers.11\n",
            "vision_model.encoder.layers.11.self_attn\n",
            "vision_model.encoder.layers.11.self_attn.k_proj\n",
            "vision_model.encoder.layers.11.self_attn.v_proj\n",
            "vision_model.encoder.layers.11.self_attn.q_proj\n",
            "vision_model.encoder.layers.11.self_attn.out_proj\n",
            "vision_model.encoder.layers.11.layer_norm1\n",
            "vision_model.encoder.layers.11.mlp\n",
            "vision_model.encoder.layers.11.mlp.activation_fn\n",
            "vision_model.encoder.layers.11.mlp.fc1\n",
            "vision_model.encoder.layers.11.mlp.fc2\n",
            "vision_model.encoder.layers.11.layer_norm2\n",
            "vision_model.post_layernorm\n",
            "visual_projection\n",
            "text_projection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to store intermediate features\n",
        "intermediate_features = {}\n",
        "\n",
        "# Hook to store the output of a specific layer\n",
        "def hook_fn(module, input, output):\n",
        "    intermediate_features[module.__class__.__name__] = output\n"
      ],
      "metadata": {
        "id": "I114e1z11WMk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you identified the image encoder module name\n",
        "for name, layer in clip_model.named_modules():\n",
        "    if \"visual_projection\" in name:  # Adjust condition based on your findings\n",
        "        print(f\"Layer: {name}, Type: {type(layer)}\")\n",
        "        layer.register_forward_hook(hook_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQVUHlae0p-2",
        "outputId": "1624d65a-0786-4864-c29a-26dff05dfe60"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: visual_projection, Type: <class 'torch.nn.modules.linear.Linear'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc55uPSv1Kg4",
        "outputId": "f452cd58-84cf-4132-eb7b-d7042bcea29a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode an image:\n",
        "#img_emb = model.encode(Image.open('2 dogs in snow.jpeg'))\n",
        "img_emb = model.encode(Image.open('assault1.jpeg'))"
      ],
      "metadata": {
        "id": "4AGeNkPP1esX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XACLeXL1wMZ",
        "outputId": "b51ea32c-8ca6-45fe-aff7-b22d40d9e650"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Linear': tensor([[-2.1673e-01,  1.5512e-01, -9.2658e-02,  7.7975e-02, -3.2581e-01,\n",
              "           3.5948e-02, -1.1605e-01,  3.3677e-01,  2.7209e-01,  1.4050e-02,\n",
              "           4.0657e-01, -1.6219e-01, -8.9801e-03, -2.7742e-01,  1.8538e-01,\n",
              "          -1.1994e-01, -1.9840e-01, -1.4667e-01,  3.0127e-01, -2.2751e-01,\n",
              "          -3.6552e-01,  3.3164e-01, -1.1231e-01,  6.9819e-01, -2.5248e-01,\n",
              "           7.7490e-01,  9.2814e-02,  2.7183e-01,  1.4320e-01, -1.8735e-01,\n",
              "           3.0996e-01,  1.2212e-01, -1.1696e-01, -1.9633e-01,  3.7455e-01,\n",
              "          -3.0558e-01,  7.1805e-01, -5.6541e-01,  1.7507e-02, -4.7425e-01,\n",
              "           1.2536e-01,  1.7086e-01, -3.7286e-01,  2.0555e-01, -7.2935e-01,\n",
              "           1.4579e-01,  2.2348e-01, -2.5001e-01,  1.5397e-02, -4.1960e-01,\n",
              "          -1.2580e+00,  8.0743e-02, -2.4432e-01, -4.2953e-01, -9.6359e-02,\n",
              "           4.4133e-01, -1.6839e-01,  9.7045e-02, -3.2540e-01,  1.8681e-01,\n",
              "          -3.3037e-01, -3.8013e-02, -2.2977e-01,  2.2510e-01,  2.7184e-02,\n",
              "           2.8837e-01, -1.7054e-02,  1.3980e-01,  3.3815e-01,  2.1156e-01,\n",
              "          -9.3667e-02, -9.6560e-02,  2.9696e-01, -3.1578e-01, -7.2203e-01,\n",
              "          -3.7784e-01, -1.0528e-01, -1.3054e-01,  3.2833e-01, -1.4563e-01,\n",
              "          -1.5190e-01,  1.4164e+00, -5.1192e-01,  3.3800e-01,  1.1804e-01,\n",
              "          -8.3018e-02, -7.9539e-03,  5.9004e-02,  2.8895e-02, -1.8741e-01,\n",
              "           9.6931e-02, -3.9746e-01,  3.0616e-02, -3.1293e-01, -2.9070e-01,\n",
              "          -4.9556e-01,  3.7739e-01,  1.0486e-01,  1.1522e-01,  6.5897e-04,\n",
              "           8.1851e-02, -2.6569e-01,  8.1087e-02, -7.5266e-01, -1.3224e-01,\n",
              "          -1.0367e-01,  2.2886e-01, -9.7496e-02,  1.2877e-03, -4.8021e-01,\n",
              "          -4.8604e-01,  5.1675e-02, -9.3845e-02,  1.8739e-01,  1.0551e-01,\n",
              "           2.0745e-01,  9.1288e-02, -8.1336e-02,  6.4544e-01,  4.4317e-01,\n",
              "           6.4153e-02, -1.3542e-01,  2.9517e-02,  3.8363e-01,  7.2935e-01,\n",
              "          -1.6304e-01, -4.9449e-01,  2.1518e-01, -2.3026e-01,  5.5964e-01,\n",
              "          -2.8651e-02,  2.8901e-01,  1.9490e-01,  2.2526e-01,  1.8782e-01,\n",
              "           1.0691e-01, -2.5043e-01,  5.1592e-01, -2.8217e-01, -1.9434e-01,\n",
              "          -4.3964e-01, -2.4173e-03,  1.1169e-01, -2.0641e-01,  5.7178e-01,\n",
              "           8.2902e-03, -4.9695e-02, -5.6356e-02,  1.6057e-01, -2.5522e-01,\n",
              "           3.4386e-01, -1.6965e-01, -3.4669e-01, -3.0441e-01,  2.3739e-01,\n",
              "          -1.9978e-01,  1.8325e-01,  5.6971e-01,  1.3839e-01, -1.7104e-01,\n",
              "          -5.5964e-03, -4.6439e-03,  1.2744e-01, -3.8726e-01, -3.4483e-01,\n",
              "          -2.9052e-01,  4.8776e-01, -2.8187e-01,  6.3220e-01,  7.9163e-02,\n",
              "          -1.6723e-01, -1.5174e-01, -1.3180e-01,  1.1375e-01, -1.0419e-01,\n",
              "           1.1779e-01,  1.3602e-01,  9.1818e-02, -5.8053e-01,  6.1505e-01,\n",
              "           1.2670e-01, -4.4749e-01,  1.2877e-01,  3.8595e-01,  4.5571e-02,\n",
              "           5.5506e-01,  3.7619e-01,  1.2153e-01, -3.1305e-01, -3.4461e-02,\n",
              "          -2.5915e-01,  1.0702e-01, -5.3191e-02,  5.1902e-01, -1.6472e+00,\n",
              "           5.0094e-01, -3.5557e-01,  2.6236e-01,  2.5966e-01,  4.8269e-01,\n",
              "           4.0986e-02, -9.5603e-02,  1.0432e-02, -2.8081e-02, -7.2045e-02,\n",
              "           2.8245e-02,  4.8515e-01, -2.1347e-01, -5.4571e-01, -2.1665e-01,\n",
              "           3.2925e-02, -1.9625e-01,  4.7372e-01, -4.3257e-01, -1.6240e-01,\n",
              "           1.5245e-02, -1.0034e-01,  1.9849e-01, -4.6367e-02, -9.4026e-02,\n",
              "          -3.0459e-01, -8.9504e-02,  4.7714e-03,  3.1580e-01, -1.4904e-01,\n",
              "           5.5727e-02, -1.3257e-01,  6.1226e-01,  9.2802e-02, -7.5004e-03,\n",
              "           2.7410e-01, -5.5288e-01,  3.0557e-01,  8.0914e-02,  4.2466e-02,\n",
              "           3.2107e-01,  2.0322e-01,  7.2618e-02, -3.3857e-02,  1.9543e-01,\n",
              "           4.6736e-01, -1.5540e-01,  4.5829e-01, -2.7653e-01, -1.1045e+00,\n",
              "           4.4078e-02,  2.8904e-01,  1.1364e-01,  4.5449e-01, -2.3452e-01,\n",
              "           4.2626e-01,  1.4985e-01, -9.9886e-02, -3.1682e-01,  8.7667e-01,\n",
              "           1.6152e-01,  1.2587e-01, -3.5535e-01,  7.2373e-02,  5.6793e-01,\n",
              "          -2.1161e-01, -1.0338e-01,  2.4914e-01, -1.6094e-01, -3.7560e-03,\n",
              "          -8.2461e-01, -4.5373e-03,  2.7678e-01, -1.9975e-01, -1.9853e-01,\n",
              "          -2.5505e-01, -7.8776e-02, -2.5128e-02, -5.5838e-02, -1.6295e-02,\n",
              "          -6.9950e-01, -1.9619e-01,  9.6176e-02,  4.8594e-01, -2.4130e-01,\n",
              "          -1.0426e+00, -2.3302e-01, -9.5038e-02,  3.3681e-01, -3.8232e-02,\n",
              "           2.7503e-01,  5.2864e-02, -5.3000e-02, -8.5473e-02,  5.9888e-01,\n",
              "          -1.6155e-01, -1.8538e-01,  7.1890e-03, -7.3619e-02, -3.6426e-02,\n",
              "           6.0351e-01,  9.4440e-02,  7.5722e-01, -3.2471e-02, -1.3171e-01,\n",
              "          -9.6088e-03, -2.4644e-01,  1.7118e-01, -3.5231e-01, -3.6357e-01,\n",
              "           3.5759e-01, -1.4729e-01, -1.5790e-01,  1.6229e-01, -4.8617e-02,\n",
              "           5.7076e-01,  1.7257e-01,  3.2307e-01,  1.3799e-01,  1.5955e-01,\n",
              "           8.6923e-02,  2.7344e-02,  1.3577e-01,  2.0087e-01,  3.6792e-02,\n",
              "          -7.2039e-02,  2.8178e-01,  3.7619e-01,  1.0038e-01,  4.2764e-01,\n",
              "           1.6718e-01, -1.0459e-01,  3.3322e-02,  2.4905e-01,  2.9007e-01,\n",
              "           3.5987e-01,  1.4566e-02,  5.3917e-02,  1.8942e-01, -2.7415e-01,\n",
              "          -4.4668e-01, -1.6241e-01, -1.3151e-01, -1.3482e-01, -2.5333e-01,\n",
              "           5.4687e-02,  1.7326e-01,  8.7346e-01,  3.3177e-01, -4.6110e-02,\n",
              "           7.8785e-02, -1.1626e-01,  5.8483e-02,  2.9971e-02, -1.6891e-01,\n",
              "           3.5165e-01, -3.2718e-01,  1.7920e-01,  2.0988e-01, -6.2831e-01,\n",
              "           2.4044e-01,  2.1855e-01, -4.9144e-01,  6.0293e-02,  8.7908e-02,\n",
              "           4.3333e-01,  2.2617e-01,  8.9864e-01, -3.0316e-02, -3.2036e-01,\n",
              "          -3.0668e-01,  4.0602e-02,  1.6900e-01, -3.0344e-02,  4.2976e-01,\n",
              "           7.5913e-02, -2.3831e-01,  1.1451e-01,  2.9975e-01, -2.7040e-01,\n",
              "           2.0124e-01,  5.8971e-01, -1.3983e-01,  4.1425e-01,  3.8272e-02,\n",
              "           1.6324e-01,  2.7799e-01, -2.3761e+00, -3.4808e-01, -2.7756e-01,\n",
              "          -3.4341e-02, -5.1012e-03, -4.9658e-01,  9.9703e-02, -1.4949e-02,\n",
              "          -2.7029e-01,  4.0897e-02,  2.4420e-01, -4.6230e-01,  2.3743e-01,\n",
              "          -3.8117e-02, -1.2800e-01,  9.9920e-02, -1.2129e-01, -3.7003e-02,\n",
              "           1.9471e-01, -2.3642e-01,  2.7404e-02, -3.1235e-01,  1.3570e-01,\n",
              "          -3.9473e-02, -4.9446e-01, -3.2866e-01, -3.5697e-01,  1.3023e-01,\n",
              "          -1.1277e-01, -6.5252e-01, -6.9938e-02,  6.0507e-02, -2.1826e-01,\n",
              "          -1.2641e-03,  3.3200e-01,  1.6749e-01, -1.4059e-01, -3.2748e-01,\n",
              "           5.6551e-02, -8.0512e-02,  1.3907e-01,  4.8478e-02,  1.1030e-01,\n",
              "           9.3097e-02, -3.5673e-01,  4.7764e-01, -2.3888e-01,  4.6539e-01,\n",
              "          -2.0389e-01, -9.2433e-02, -8.0589e-02, -1.5168e-01, -2.3828e-01,\n",
              "           5.6323e-02,  2.8432e-01, -1.0237e-01, -1.6128e-01,  1.8873e-01,\n",
              "          -1.8928e-01, -2.7174e-02,  4.5970e+00, -3.2294e-01, -6.3788e-02,\n",
              "           4.6202e-02,  1.3448e-01,  4.8317e-01,  1.8112e-01,  2.8281e-02,\n",
              "           5.0219e-02,  1.6568e-01, -5.9771e-02,  2.8741e-01,  9.8156e-02,\n",
              "          -3.8498e-02, -7.0468e-02,  2.0425e-01,  1.2057e-01, -2.9729e-02,\n",
              "           8.6067e-02, -2.3947e-01,  7.0050e-02, -7.0567e-01, -3.8660e-02,\n",
              "          -2.8116e-01, -5.4582e-01, -3.0806e-01,  1.5724e-01, -1.5420e-01,\n",
              "           4.2997e-01,  3.3541e-01, -3.6805e-01,  5.9146e-01, -1.4913e-01,\n",
              "           1.0028e+00,  1.9010e-01,  4.9441e-02,  5.1400e-01, -9.0165e-02,\n",
              "          -2.7307e-01, -2.3229e-01,  5.5073e-01, -3.2597e-01, -2.5400e-01,\n",
              "          -6.2205e-02, -9.2870e-02, -6.9803e-02, -4.7995e-01, -6.3989e-01,\n",
              "           1.5395e-01, -2.6005e-01, -6.8685e-01,  1.4790e-01, -7.3645e-02,\n",
              "          -1.3706e-01,  4.2325e-02, -6.7835e-01, -1.7089e-02,  1.3661e-01,\n",
              "          -3.0118e-01, -1.4966e-02,  1.1523e-01, -2.2374e-01,  3.4801e-01,\n",
              "           5.8919e-01,  1.0277e-01, -2.5705e-01,  2.7061e-01, -3.7109e-01,\n",
              "           2.4134e-01,  1.1520e-01]])}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qiZblo8l1x6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}